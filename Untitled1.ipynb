{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Project: Build a Traffic Sign Recognition Program\n",
    "\n",
    "**Build a Traffic Sign Recognition Project**\n",
    "\n",
    "### The goals / steps of this project are the following:\n",
    "* Load the data set (see below for links to the project data set)\n",
    "* Explore, summarize and visualize the data set\n",
    "* Extending data set to make better model\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/visualization.jpg \"Visualization\"\n",
    "[image2]: ./examples/grayscale.jpg \"Grayscaling\"\n",
    "[image3]: ./examples/random_noise.jpg \"Random Noise\"\n",
    "[image4]: ./sample_images/keepleft.png \"Merge Sign 1\"\n",
    "[image5]: ./sample_images/circulate.png \"Traffic Sign 2\"\n",
    "[image6]: ./sample_images/km_30.png \"Traffic Sign 3\"\n",
    "[image7]: ./sample_images/stop1.png \"Traffic Sign 4\"\n",
    "[image8]: ./sample_images/row.png   \"Traffic Sign 5\"\n",
    "[image9]: ./cross_entropy.png      \"Cross entropy Result\"\n",
    "\n",
    "\n",
    "### Data Set Summary & Preprocessing\n",
    "\n",
    "#### 1.Data Set Summary\n",
    "\n",
    "I used the pandas library to calculate summary statistics of the traffic\n",
    "signs data set:\n",
    "\n",
    "* The size of training set is 34799\n",
    "* The size of test set is 12630\n",
    "* The shape of a traffic sign image is 32,32,3\n",
    "* The number of unique classes/labels in the data set is 43\n",
    "\n",
    "#### 2.Preprocessing\n",
    "* On the preprocessing step I tried two different method. \n",
    "    (1) normalize data after gray scaling.\n",
    "    (2) first gray scailing and then normalize data\n",
    "* In my experience first method show much better performance\n",
    "\n",
    "* Also I compare two python function to normalize image\n",
    "    (1)sklearn.preprocessing.normalize\n",
    "    (2) cv2.equalizeHist \n",
    "* Two function give similar result but cv2.equalizeHist was much more comfirtable to implement on the image data\n",
    "![alt text][image2]\n",
    "\n",
    "### Design and Test a Model Architecture\n",
    "\n",
    "#### 1. Model searching\n",
    "\n",
    "* I tried to compare distinct model structure based on 10 epoches results. many stuck at local optimum in first 10 steps so it actually help identifying good models. \n",
    "\n",
    "* I considered overall structure of model, Location of dropout layer, implementing batch norm layer, implementation of inception layer \n",
    "\n",
    "* I got idea of final model in Pierre Sermanet and Yann LeCun's research\n",
    "\n",
    "#### 2. Slim\n",
    "* I found out we can easily implement batch normalization and other technique with slim library of tensorflow\n",
    "\n",
    "\n",
    "#### 3. Final Model\n",
    "\n",
    "The code for my final model is located in the 16th cell of the ipython notebook. \n",
    "\n",
    "My final model consisted of the following layers:\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t   | \n",
    "|:---------------------:|:-----------------------------------------------: | \n",
    "| Input         \t\t| 32x32x1 processed image   \t\t\t\t\t   | \n",
    "| first Convolution    \t| Valid padding,activation and pooling             |\n",
    "| Second Convolution    | Valid padding,activation and pooling,connected   |\n",
    "| conv layer beta       | max pooling first convolution                    |\n",
    "| conv layer gamma\t    | connect cnn filter to conv layer beta\t\t\t   | \t\t\t\t\n",
    "| fc_0           \t    | layer connecting cnv_layer gamma and second cnn  |\n",
    "| fc_1           \t    | fully connect fc_0      \t\t\t\t\t\t   |\n",
    "| fc_2          \t\t| fullly connect fc_2        \t\t\t\t\t   |\n",
    "| fc_3   \t\t\t\t| Final layer   \t\t\t\t\t\t\t       |\n",
    "\n",
    "I used tf.concat to connect the two seperate convolutional layers\n",
    "\n",
    "\n",
    "#### 4. Model Accuracy \n",
    "The code for calculating the accuracy of the model is located in the ninth cell of the Ipython notebook.\n",
    "\n",
    "My final model results were:\n",
    "* training set accuracy of 90.3%\n",
    "* validation set accuracy of 92 %\n",
    "* test set accuracy of 92.1%\n",
    "\n",
    "\n",
    "### Test a Model on New Images\n",
    "\n",
    "#### 1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n",
    "\n",
    "Here are five German traffic signs that I found on the web:\n",
    "\n",
    "![alt text][image4] ![alt text][image5] ![alt text][image6] \n",
    "![alt text][image7] ![alt text][image8]\n",
    "\n",
    "The first image might be difficult to classify because ...\n",
    "\n",
    "#### 2.\n",
    "\n",
    "The code for making predictions on my final model is located in the tenth cell of the Ipython notebook.\n",
    "\n",
    "Here are the results of the prediction:\n",
    "\n",
    "| Image\t\t\t        |     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Stop Sign      \t\t| Stop sign   \t\t\t\t\t\t\t\t\t| \n",
    "| Round-about Mandatory | 30khm limit \t\t\t\t\t\t\t\t\t|\n",
    "| Yield\t\t\t\t\t| Yield\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| 60 km/h\t      \t\t| Non-entry   \t\t\t\t\t \t\t\t\t|\n",
    "| Right of way at next. | Right of way at next.   \t\t\t\t\t\t|\n",
    "\n",
    "\n",
    "The model was able to correctly guess 4 of the 5 traffic signs, which gives an accuracy of 60%. \n",
    "Although It show us the much worse result than german traffic dataset. We need further investigation more bigger traning set. \n",
    "\n",
    "#### 3. Predicted Probabilty on test data set\n",
    "![alt text][image9] \n",
    "\n",
    "* We can verify that our model get wrong answer on image which have getty images label. We can verify that the part which human's ignore can make crucial difference to the model. \n",
    "* It will be limit of our model\n",
    "\n",
    "### Mistake I made. \n",
    "\n",
    "\n",
    "#### 1. Shuffle images during training model . \n",
    "* Shuffling images during training within epoch will stop your model to improve. I fight my-self to handle this problem. \n",
    "\n",
    "\n",
    "#### 2. Save model in same directory. \n",
    "* If you save model using saver object in tensorflow it create three files containing checkpoint. unlike other file checkpoint is created in same name. It will be better for you to save each model in different directory to \n",
    "\n",
    "#### 3. Using CPU. \n",
    "* Thinking of other practice we did with Le-Net and other model I think I can make a model can be trained perfectly with-in few epoches. How ever It was not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = './traffic-signs-data/train.p' \n",
    "validation_file= './traffic-signs-data/valid.p' \n",
    "testing_file =  './traffic-signs-data/test.p' \n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.pipeline import make_pipeline,make_union\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import warp\n",
    "from skimage.transform import ProjectiveTransform\n",
    "\n",
    "def grayscale(img_array):\n",
    "    gray_img = np.empty_like(np.add.reduce(img_array,3))\n",
    "    for i, x in enumerate(img_array):\n",
    "        gray_img[i] = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_img\n",
    "\n",
    "def processing1(img_array):\n",
    "    scaled_img = grayscale(img_array)\n",
    "    scaled_img = scaled_img.astype('float32')\n",
    "    scaled_img = scaled_img.reshape(scaled_img.shape[0],scaled_img.shape[1],scaled_img.shape[2],1)\n",
    "    return scaled_img\n",
    "\n",
    "\n",
    "def flip_extend(X, y):\n",
    "    self_flippable_horizontally = np.array([11, 12, 13, 15, 17, 18, 22, 26, 30, 35])\n",
    "    \n",
    "    self_flippable_vertically = np.array([1, 5, 12, 15, 17])\n",
    "    \n",
    "    self_flippable_both = np.array([32, 40])\n",
    "    \n",
    "    cross_flippable = np.array([\n",
    "        [19, 20], \n",
    "        [33, 34], \n",
    "        [36, 37], \n",
    "        [38, 39],\n",
    "        [20, 19], \n",
    "        [34, 33], \n",
    "        [37, 36], \n",
    "        [39, 38],   \n",
    "    ])\n",
    "    \n",
    "    num_classes = 43\n",
    "    \n",
    "    X_extended = np.empty([0, X.shape[1], X.shape[2], X.shape[3]], dtype = X.dtype)\n",
    "    y_extended = np.empty([0], dtype = y.dtype)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # First copy existing data for this class\n",
    "        X_extended = np.append(X_extended, X[y == c], axis = 0)\n",
    "        # If we can flip images of this class horizontally and they would still belong to said class...\n",
    "        if c in self_flippable_horizontally:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X[y == c][:, :, ::-1, :], axis = 0)\n",
    "        # If we can flip images of this class horizontally and they would belong to other class...\n",
    "        if c in cross_flippable[:, 0]:\n",
    "            # ...Copy flipped images of that other class to the extended array.\n",
    "            flip_class = cross_flippable[cross_flippable[:, 0] == c][0][1]\n",
    "            X_extended = np.append(X_extended, X[y == flip_class][:, :, ::-1, :], axis = 0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype = int))\n",
    "        \n",
    "        # If we can flip images of this class vertically and they would still belong to said class...\n",
    "        if c in self_flippable_vertically:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X_extended[y_extended == c][:, ::-1, :, :], axis = 0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype = int))\n",
    "        \n",
    "        # If we can flip images of this class horizontally AND vertically and they would still belong to said class...\n",
    "        if c in self_flippable_both:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X_extended[y_extended == c][:, ::-1, ::-1, :], axis = 0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype = int))\n",
    "    \n",
    "    return (X_extended, y_extended)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train = processing1(X_train)\n",
    "X_valid = processing1(X_valid)\n",
    "X_train,y_train = shuffle(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_extended = np.load('./X_train_extended.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "weights = {'wc1': tf.Variable(tf.truncated_normal(shape = (5,5,1,6), mean = 0,stddev = 0.1)),\n",
    "          'wc2': tf.Variable(tf.truncated_normal(shape = (5,5,6,16), mean = 0,stddev = 0.1)),\n",
    "          'wd1': tf.Variable(tf.truncated_normal(shape = (400,120), mean = 0,stddev = 0.1)),\n",
    "          'wd2': tf.Variable(tf.truncated_normal(shape = (120,84), mean = 0,stddev = 0.1)),\n",
    "          'wd3': tf.Variable(tf.truncated_normal( shape = (84,43), mean = 0,stddev = 0.1)),\n",
    "          'wc1_gamma': tf.Variable(tf.truncated_normal(shape = (3,3,6,32), mean = 0 ,stddev= 0.1)),\n",
    "          'wc2_beta' : tf.Variable(tf.truncated_normal(shape = (1,1,16,32) ,mean = 0 ,stddev= 0.1)),\n",
    "          'bd_flat': tf.Variable(tf.truncated_normal(shape = (1600,120),mean = 0, stddev = 0.1 ))}\n",
    "\n",
    "biases = { 'bc1' : tf.Variable(tf.random_normal([6])),\n",
    "           'bc2' : tf.Variable(tf.random_normal([16])),\n",
    "         'bd1' : tf.Variable(tf.random_normal([120])),\n",
    "         'bd2' : tf.Variable(tf.random_normal([84])),\n",
    "         'bd3' : tf.Variable(tf.random_normal([43])),\n",
    "         'wc1_gamma': tf.Variable(tf.random_normal([32])),\n",
    "         'wc2_beta' : tf.Variable(tf.random_normal([32]))}\n",
    "\n",
    "def batch_norm_layer(x, train_phase):\n",
    "    bn_train = tf.contrib.layers.batch_norm(x, center=True, scale=True, is_training=True)\n",
    "    bn_inference = tf.contrib.layers.batch_norm(x, center=True, scale=True, is_training=False)\n",
    "    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "    return bn\n",
    "\n",
    "def drop_out_layer(x,train_phase):\n",
    "    dropout_train = tf.nn.dropout(x,0.6)\n",
    "    return tf.cond(train_phase, lambda : dropout_train, lambda: x)\n",
    "\n",
    "def cnl_layer(x,conv_W,bias_weight,activation = 1 ):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    cnl    = tf.nn.bias_add(tf.nn.conv2d(x,conv_W, strides=[1, 1, 1, 1], padding='VALID'),bias_weight) \n",
    "    \n",
    "    # RELU for Activation.\n",
    "    if activation == 1: \n",
    "        cnl = tf.nn.relu(cnl)\n",
    "    \n",
    "    return cnl\n",
    "def pooled_convlayer(x,conv_W,bias_weight,batch_norm =False,drop_out =False,training_phase = True):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    cnl    = tf.nn.bias_add(tf.nn.conv2d(x,conv_W, strides=[1, 1, 1, 1], padding='VALID'),bias_weight) \n",
    "    if batch_norm: \n",
    "        cnl = batch_norm_layer(cnl,training_phase)\n",
    "        \n",
    "    # RELU for Activation.\n",
    "    cnl = tf.nn.relu(cnl)\n",
    "    if drop_out:\n",
    "        cnl = drop_out_layer(cnl,training_phase)\n",
    "    #Pooling\n",
    "    cnl = tf.nn.max_pool(cnl, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    return cnl\n",
    "\n",
    "def full_connection_layer(x,fully_W, bias_weight,activation = 1, drop_out = False,training_phase= True):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    fc   = tf.nn.bias_add(tf.matmul(x, fully_W) ,bias_weight)\n",
    "    \n",
    "    # RELU for Activation.\n",
    "    if activation == 1 :\n",
    "        fc = tf.nn.relu(fc)\n",
    "        if drop_out == True:\n",
    "            fc = drop_out_layer(fc,training_phase)\n",
    "    return fc\n",
    "def Lenet(x):\n",
    "    # Layer 1. Convolutional. Input = 32x32x1. Output = 14x14x6\n",
    "    cnv_layer_1 = pooled_convlayer(x,weights['wc1'],biases['bc1'])\n",
    "    # Layer 2 Convolutional. Input = 14x14x6. Output = 5x5x16\n",
    "    cnv_layer_2 = pooled_convlayer(cnv_layer_1, weights['wc2'],biases['bc2'])\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc_1 = tf.contrib.layers.flatten(cnv_layer_2)\n",
    "    fc_1 = full_connection_layer(fc_1,weights['wd1'],biases['bd1'])\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc_2 = full_connection_layer(fc_1,weights['wd2'],biases['bd2'])\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc_3 = full_connection_layer(fc_2,weights['wd3'],biases['bd3'],activation=0)\n",
    "    return fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rate = 0.001\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1),name='x')\n",
    "y = tf.placeholder(tf.int32, (None),name = 'y')\n",
    "one_hot_y  = tf.one_hot(y,43)\n",
    "logits = Lenet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels = one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate = rate) \n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Lenet5(x,is_training):\n",
    "    x = drop_out_layer(x) \n",
    "    cnv_layer_1 = pooled_convlayer(x,weights['wc1'],biases['bc1'])\n",
    "    \n",
    "    cnv_layer_2 = pooled_convlayer(cnv_layer_1, weights['wc2'],biases['bc2'])\n",
    "\n",
    "    cnv_layer_1_beta = tf.nn.max_pool(cnv_layer_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    cnv_layer_1_gamma   = cnl_layer(cnv_layer_1_beta,weights['wc1_gamma'],biases['wc1_gamma'],activation = 0)\n",
    "    \n",
    "    cnv_layer_2_beta = cnl_layer(cnv_layer_2,weights['wc2_beta'],biases['wc2_beta'],activation = 0)\n",
    "    \n",
    "    new_conv = tf.concat([cnv_layer_1_gamma,cnv_layer_2_beta],1)\n",
    "    \n",
    "    fc_0 = tf.contrib.layers.flatten(new_conv)\n",
    "\n",
    "    fc_1 = full_connection_layer(fc_0,weights['bd_flat'],biases['bd1'],training_phase = is_training)\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc_2 = full_connection_layer(fc_1,weights['wd2'],biases['bd2'],training_phase = is_training)\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc_3 = full_connection_layer(fc_2,weights['wd3'],biases['bd3'],activation=0)\n",
    "    return fc_3\n",
    "\n",
    "rate = 0.001\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1),name='x')\n",
    "y = tf.placeholder(tf.int32, (None),name = 'y')\n",
    "one_hot_y  = tf.one_hot(y,43)\n",
    "    \n",
    "is_training = tf.placeholder(tf.bool) \n",
    "logits = Lenet5(x,is_training)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels = one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdagradOptimizer(rate) \n",
    "training_operation = optimizer.minimize(loss_operation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.184 Traing Accuracy  = 0.202\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.242 Traing Accuracy  = 0.293\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.280 Traing Accuracy  = 0.354\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.307 Traing Accuracy  = 0.393\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.336 Traing Accuracy  = 0.428\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.359 Traing Accuracy  = 0.457\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.386 Traing Accuracy  = 0.481\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.404 Traing Accuracy  = 0.503\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.426 Traing Accuracy  = 0.525\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.446 Traing Accuracy  = 0.543\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    #saver = tf.train.import_meta_graph('lenet5.meta')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Training...\")\n",
    "    for i in range(10):\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, is_training : True})\n",
    "\n",
    "        training_accuracy = evaluate(X_train,y_train)    \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy),\n",
    "              \"Traing Accuracy  = {:.3f}\".format(training_accuracy))\n",
    "        print()\n",
    "\n",
    "    saver.save(sess, './lenet5_2')\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11a14af28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE2dJREFUeJzt3W+MXNV5x/Hvs2bXrDCyTQGzMqZ2EDKgqFnQyrKgityk\njSiKBEhNBC8QL1A2qoJUS+kLoFKhUl+QqmDxypUpFk5F+dPwJ6hCaRAKQpGAYKhtTEwDQUtiWPxH\nGNsF26y9T1/MtbS4c87MnLn3zG7O7yOtdvaeufc+c2eevTP3mXOOuTsiUp6hQQcgIoOh5BcplJJf\npFBKfpFCKflFCqXkFymUkl+kUEp+kUIp+UUKdVY/K5vZdcCDwCLgX939vtj9R0ZGfHR0tG3b7Oxs\ncL0lS5a0XX7OOecE1zl69Giw7dSpU8G2mJmZmbbLY7HHvkG5YsWKYNvBgweDbZ999lmwLSQWY05D\nQ/Wfb8ys53VSv9m6aNGipPXqFjqOMzMznDx5sqsDYqkHwcwWAb8B/gLYC7wO3OLuvw6ts3TpUr/m\nmmvath0/fjy4r/Xr1/e0HOCll14Ktn366afBtpiPP/647fJY7LG2jRs3BtseeeSRYNsrr7wSbAs5\nceJEz+s0YfHixbVvM3RCiTl27FjSvpYvX560Xt3OPvvstsunpqY4duxYV8nfz7/hdcB77v6+u38B\nPA7c0Mf2RCSjfpJ/JfD7OX/vrZaJyALQz2f+dm8t/t9nCDObBCYh/FZFRPLr58y/F1g15++LgY/O\nvJO7b3H3CXefGBkZ6WN3IlKnfpL/deAyM1tjZiPAzcBz9YQlIk1Lftvv7ifN7A7gv2iV+ra6+9ux\ndcws6Wpv6Ip57Er66tWrg21TU1M9xwDhjy2xOHbs2JEUR2ybKVKvstddJYhtLzXG2JX7lEpATN3P\nS0zsY3Iojl5Kun3V+d39eeD5frYhIoOhb/iJFErJL1IoJb9IoZT8IoVS8osUqq+r/b0aGhpKKr2E\nSh6xDjqhTjid1osJlVdi24u1Pfvss8G2WPwxKaW51BJbrOyV8m3O+dL5KCb2fC5btqzWfaWU+nqh\nM79IoZT8IoVS8osUSskvUiglv0ihsl7tn52dDXbCiF3pDXWO2bNnT3CdWFUhdlU2dhU1dKU3dmU+\nZXud1st5VTxWCYi15YwxZ2ebJoYhS6mMhNbpZYxEnflFCqXkFymUkl+kUEp+kUIp+UUKpeQXKVTW\nUt/ixYtZs2ZN27Z33nknuF5qJ5eQiy66KKktVOaJlQ5Ty4B1D3OeWqJKHQMvtF7qTDmx0qGGhE+j\nM79IoZT8IoVS8osUSskvUiglv0ihlPwiheqr1GdmU8BR4BRw0t0nYvdftWoVmzZtatsWK+WEer+l\nTpP16quvJq2XEkdMrESVOn5bqOy4EMphqb0cY+XD0OuqiTJr3dOGpbyusk3XVfkzdz9Yw3ZEJCO9\n7RcpVL/J78DPzewNM5usIyARyaPft/3XuvtHZnYh8IKZvePuL8+9Q/VPYRLgkksu6XN3IlKXvs78\n7v5R9Xs/8Aywrs19trj7hLtPXHDBBf3sTkRqlJz8ZnaOmZ17+jbwLWB3XYGJSLP6edu/AnjGzE5v\n59/d/WexFT788EPuuuuutm0pZZ7UEtv4+HiwLdZDL1QGTO2pljq9U8oApLFj1cQAmKFyWayMlno8\nYvFPT0/3vL3UgUlj66W+RkLqKN0mJ7+7vw98re8IRGQgVOoTKZSSX6RQSn6RQin5RQql5BcpVNYB\nPA8cOMDmzZvbtsV6PaXM7xcbiDNnD7fUATBTe7jVLbV3Yd3l2dQekGNjY22Xx0qAqeW8mJS5I2Ov\n4ZCDB7vvY6czv0ihlPwihVLyixRKyS9SKCW/SKGyXu2PSRmHLebQoUPBttjUYJdffnmwbfXq1W2X\nx6bkil2Jjl3Rr7sjSGrVIWdlISa1Y1LoSnqoCgB5nxcIv0ZSXovvvvtu1/vVmV+kUEp+kUIp+UUK\npeQXKZSSX6RQSn6RQmUt9c3OziaV7UJiJZ5YB4xYKSfWmSKls0qsDBizfPnypPViJc6cQuWr3KXD\n0P5i4wWmjiUYe13FSq2hGGNl4lCpr5eORzrzixRKyS9SKCW/SKGU/CKFUvKLFErJL1KojqU+M9sK\nfBvY7+5frZadBzwBrAamgO+6e8cak7snlTVCYuscPnw42BYrN4am5IJwKSe1nBfrtRUrOcZKSqEe\ni02U2GI93HKOk5gidgxTy4CpvQFDxyr2ugo9z730OuzmzP8IcN0Zy+4EXnT3y4AXq79FZAHpmPzu\n/jLwyRmLbwC2Vbe3ATfWHJeINCz1M/8Kd58GqH5fWF9IIpJD41/vNbNJYLLp/YhIb1LP/PvMbAyg\n+r0/dEd33+LuE+4+kbgvEWlAavI/B9xW3b4N+Gk94YhILt2U+h4DNgDnm9le4B7gPuBJM7sd+B3w\nnX4DmS8DRU5NTdW6vVjJa+nSpcG2UK8tSJvKq+7HNZ+kTCmWOrBqbL1YGTBWggvFGCs7h57PI0eO\nBNc5U8fkd/dbAk3f7HovIjLv6Bt+IoVS8osUSskvUiglv0ihlPwihco6gOfo6Chr167teb1QKSRW\nkmlC3eXIDz74INgWm0suVlIaHx9vuzxWoorNXZiqzt6bse11aqtzHYj3toxJmeMvpaQ7MzPT9fZ1\n5hcplJJfpFBKfpFCKflFCqXkFymUkl+kUFlLfUNDQ7UO1NnEIJEpJaDUOGKlnFiPrlA5D8JlwFgv\nwZhYb8C6S5+pg5bmLPXF1ss5aGloX0ND3Z/PdeYXKZSSX6RQSn6RQin5RQql5BcpVNar/XVLvfIa\n6xgTmyIpZ2eVWByxSkBoCrDYlfT5UglIjSPWMSlUJUgZ969TW2ybo6OjwbaU7YXazKzr7evML1Io\nJb9IoZT8IoVS8osUSskvUiglv0ihupmuayvwbWC/u3+1WnYv8D3gQHW3u939+U7bWrRoUbDMljIe\nXxMltvmwvU7bTCmxrV+/PrhOrPQZK78tXrw42LZz585gW4rU6ctCbbk79sTG8AuVAVNKju4eXOdM\n3Zz5HwGua7N8k7uPVz8dE19E5peOye/uLwOfZIhFRDLq5zP/HWa2y8y2mtny2iISkSxSk38zcCkw\nDkwD94fuaGaTZrbdzLZ/8cUXibsTkbolJb+773P3U+4+CzwErIvcd4u7T7j7xMjISGqcIlKzpOQ3\ns7nTydwE7K4nHBHJpZtS32PABuB8M9sL3ANsMLNxwIEp4Pvd7GzZsmXceOONbdti5atQL7bU6bpy\njrWWqu5xAeseE7CTEydOtF0ee55j5bCcZd0m4kjZX6wnYOhxzc7Odr3fjsnv7re0Wfxw13sQkXlJ\n3/ATKZSSX6RQSn6RQin5RQql5BcpVNYBPN09WKLYsGFDcL1Q+So2cGMTQmWe1B5iOctXqQOCppYB\nQwOJxnoCLl8e/pZ47DHHtpnynMVKbKnPWWybsdJik3TmFymUkl+kUEp+kUIp+UUKpeQXKZSSX6RQ\nWUt9n3/+ebCsFCs3hXqCxXr1NdH7KqXE1kQcKdvMOS8ghOcGvOKKK4LrxMQec6gHITQzuGpI3c91\nrAQYajt16lTX29eZX6RQSn6RQin5RQql5BcplJJfpFBZr/YfP3482BknZWqi3FLGWotpokNQaJt1\njwkI8fH4QvsLVQE6Se3YU7fYcYwdq9iV+1i1ole9jOGnM79IoZT8IoVS8osUSskvUiglv0ihlPwi\nhepmuq5VwI+Bi4BZYIu7P2hm5wFPAKtpTdn1XXc/lBpI3eW81Km8YuPSxcaYC4k9riY6/YTiX7t2\nbXCdlMcF8fhTpvnKeaxi22uiLBqTUqoMlZcPHjzY9Ta6OfOfBH7o7lcA64EfmNmVwJ3Ai+5+GfBi\n9beILBAdk9/dp939zer2UWAPsBK4AdhW3W0b0H4GThGZl3r6zG9mq4GrgNeAFe4+Da1/EMCFdQcn\nIs3p+uu9ZrYEeArY6O5HzKzb9SaBSYCRkZGUGEWkAV2d+c1smFbiP+ruT1eL95nZWNU+Buxvt667\nb3H3CXefGB4eriNmEalBx+S31in+YWCPuz8wp+k54Lbq9m3AT+sPT0Sa0s3b/muBW4G3zOz0gG53\nA/cBT5rZ7cDvgO902tDs7GytJb0mevullJtS40gtbaWsF+s5FivLxdpSHnfqsYqV0Q4fPlxrHKnH\nflDTbqXqmPzu/ksg9AH/m/WGIyK56Bt+IoVS8osUSskvUiglv0ihlPwihco6gKe7B0slqT2icgqV\nvXIPMJpSiooNtplz2rMmjlXKaye1R2JsX6kDuaYIxT801P35XGd+kUIp+UUKpeQXKZSSX6RQSn6R\nQin5RQqVtdSXU6yHVawkkzJnYGppKLavQ4fCY6GmlJRi22uizFp3D8iUfcXamnjOFkJZdC6d+UUK\npeQXKZSSX6RQSn6RQin5RQq1oK/2p065VPc4cqmdRJq4mhuqcjTR6aSJY1y3nJ2xUrcZOo5NTOc2\nl878IoVS8osUSskvUiglv0ihlPwihVLyixSqY6nPzFYBPwYuAmaBLe7+oJndC3wPOFDd9W53fz41\nkJSyUe5OIiliMcbKgKnbzKnuOFJLtzmnFGu6/JZTN3X+k8AP3f1NMzsXeMPMXqjaNrn7PzcXnog0\npZu5+qaB6er2UTPbA6xsOjARaVZPn/nNbDVwFfBategOM9tlZlvNbHnNsYlIg7pOfjNbAjwFbHT3\nI8Bm4FJgnNY7g/sD602a2XYz237y5MkaQhaROnSV/GY2TCvxH3X3pwHcfZ+7n3L3WeAhYF27dd19\ni7tPuPvEWWct6K4EIn9QOia/mRnwMLDH3R+Ys3xszt1uAnbXH56INKWbU/G1wK3AW2a2o1p2N3CL\nmY0DDkwB328kwgbU3RstdXtNTJMVKnuljtNXd9krtZwXkzLuYkxqjDl7OYb2tW/fvq630c3V/l8C\n1qYpuaYvIoOnb/iJFErJL1IoJb9IoZT8IoVS8osUKuu3bswsaRqn+TL1U8o6dZehUqX2fKu75Jj6\nmBd6T8aU14gG8BSRRij5RQql5BcplJJfpFBKfpFCKflFCrWgO9jnngev7jJgE+ouG8V6A4bmBYxJ\nLYvm7E03X+bxS1nH3bu+r878IoVS8osUSskvUiglv0ihlPwihVLyixRq3pT66i6J5S4ppeyrCTlL\ni6Ojoz3H0USvvpyDdObs1dc0nflFCqXkFymUkl+kUEp+kUIp+UUK1fFqv5mdDbwMLK7u/xN3v8fM\n1gCPA+cBbwK3uvsXsW0NDQ3VejW6ibHncnfEqVuuaaGkN3VXJEJaU2t2p5sz/wngG+7+NVrTcV9n\nZuuBHwGb3P0y4BBwe8+RisjAdEx+b/nf6s/h6seBbwA/qZZvA25sJEIRaURXn/nNbFE1Q+9+4AXg\nt8Cn7n6yusteYGUzIYpIE7pKfnc/5e7jwMXAOuCKdndrt66ZTZrZdjPbPjMzkx6piNSqp6v97v4p\n8BKwHlhmZqcvGF4MfBRYZ4u7T7j7xPDwcD+xikiNOia/mV1gZsuq26PAnwN7gF8Af1Xd7Tbgp00F\nKSL166ZjzxiwzcwW0fpn8aS7/6eZ/Rp43Mz+Efhv4OFOG0qdritkoZehYo85Nj5hzjjq7oiz0J+z\nmNTHFjpWTZSy5+qY/O6+C7iqzfL3aX3+F5EFSN/wEymUkl+kUEp+kUIp+UUKpeQXKZT1Mr1P3zsz\nOwB8UP15PnAw287DFMeXKY4vW2hx/LG7X9DNBrMm/5d2bLbd3ScGsnPFoTgUh972i5RKyS9SqEEm\n/5YB7nsuxfFliuPL/mDjGNhnfhEZLL3tFynUQJLfzK4zs/8xs/fM7M5BxFDFMWVmb5nZDjPbnnG/\nW81sv5ntnrPsPDN7wczerX4vH1Ac95rZh9Ux2WFm12eIY5WZ/cLM9pjZ22b2N9XyrMckEkfWY2Jm\nZ5vZr8xsZxXHP1TL15jZa9XxeMLMRvrakbtn/QEW0RoG7CvACLATuDJ3HFUsU8D5A9jv14Grgd1z\nlv0TcGd1+07gRwOK417gbzMfjzHg6ur2ucBvgCtzH5NIHFmPCWDAkur2MPAarQF0ngRurpb/C/DX\n/exnEGf+dcB77v6+t4b6fhy4YQBxDIy7vwx8csbiG2gNhAqZBkQNxJGdu0+7+5vV7aO0BotZSeZj\nEokjK29pfNDcQST/SuD3c/4e5OCfDvzczN4ws8kBxXDaCnefhtaLELhwgLHcYWa7qo8FjX/8mMvM\nVtMaP+I1BnhMzogDMh+THIPmDiL5280qMKiSw7XufjXwl8APzOzrA4pjPtkMXEprjoZp4P5cOzaz\nJcBTwEZ3P5Jrv13Ekf2YeB+D5nZrEMm/F1g15+/g4J9Nc/ePqt/7gWcY7MhE+8xsDKD6vX8QQbj7\nvuqFNws8RKZjYmbDtBLuUXd/ulqc/Zi0i2NQx6Tad8+D5nZrEMn/OnBZdeVyBLgZeC53EGZ2jpmd\ne/o28C1gd3ytRj1HayBUGOCAqKeTrXITGY6JteaYehjY4+4PzGnKekxCceQ+JtkGzc11BfOMq5nX\n07qS+lvg7wYUw1doVRp2Am/njAN4jNbbxxla74RuB/4IeBF4t/p93oDi+DfgLWAXreQbyxDHn9J6\nC7sL2FH9XJ/7mETiyHpMgD+hNSjuLlr/aP5+zmv2V8B7wH8Ai/vZj77hJ1IofcNPpFBKfpFCKflF\nCqXkFymUkl+kUEp+kUIp+UUKpeQXKdT/AdlVXgmIGgOKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a01dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from sklearn.pipeline import make_pipeline,make_union\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "def grayscale(img_array):\n",
    "    gray_img = np.empty_like(np.add.reduce(img_array,3))\n",
    "    for i, x in enumerate(img_array):\n",
    "        gray_img[i] = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_img\n",
    "\n",
    "scaled_img =grayscale(X_train)\n",
    "scaled_img = scaled_img.astype('float32')\n",
    "X_valid_scaled = grayscale(X_valid)\n",
    "X_valid_scaled = X_valid_scaled.astype('float32')\n",
    "scaled_img = scaled_img.astype('float32')\n",
    "scaled_img2 = normalize(scaled_img.reshape(scaled_img.shape[0],scaled_img.shape[1]*scaled_img.shape[1]))\\\n",
    "             .reshape(scaled_img.shape[0],scaled_img.shape[1], scaled_img.shape[1])\n",
    "X_valid_scaled2 = normalize(X_valid_scaled.reshape(4410,32*32)).reshape(4410,32,32)\n",
    "X_valid_scaled = X_valid_scaled.astype('float32')\n",
    "plt.imshow(scaled_img2[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaled_img2 = scaled_img2.reshape(34799, 32, 32,1)\n",
    "X_valid_scaled2 = X_valid_scaled2.reshape(4410, 32, 32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "weights = {'wc1': tf.Variable(tf.truncated_normal(shape = (5,5,1,6), mean = 0,stddev = 0.1)),\n",
    "          'wc2': tf.Variable(tf.truncated_normal(shape = (5,5,6,16), mean = 0,stddev = 0.1)),\n",
    "          'wd1': tf.Variable(tf.truncated_normal(shape = (400,120), mean = 0,stddev = 0.1)),\n",
    "          'wd2': tf.Variable(tf.truncated_normal(shape = (120,84), mean = 0,stddev = 0.1)),\n",
    "          'wd3': tf.Variable(tf.truncated_normal( shape = (84,43), mean = 0,stddev = 0.1)),\n",
    "          'wc1_gamma': tf.Variable(tf.truncated_normal(shape = (3,3,6,32), mean = 0 ,stddev= 0.1)),\n",
    "          'wc2_beta' : tf.Variable(tf.truncated_normal(shape = (1,1,16,32) ,mean = 0 ,stddev= 0.1)),\n",
    "          'bd_flat': tf.Variable(tf.truncated_normal(shape = (1600,120),mean = 0, stddev = 0.1 ))}\n",
    "\n",
    "biases = { 'bc1' : tf.Variable(tf.random_normal([6])),\n",
    "           'bc2' : tf.Variable(tf.random_normal([16])),\n",
    "         'bd1' : tf.Variable(tf.random_normal([120])),\n",
    "         'bd2' : tf.Variable(tf.random_normal([84])),\n",
    "         'bd3' : tf.Variable(tf.random_normal([43])),\n",
    "         'wc1_gamma': tf.Variable(tf.random_normal([32])),\n",
    "         'wc2_beta' : tf.Variable(tf.random_normal([32]))}\n",
    "\n",
    "\n",
    "def cnl_layer(x,conv_W,bias_weight,activation = 0):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    cnl    = tf.nn.bias_add(tf.nn.conv2d(x,conv_W, strides=[1, 1, 1, 1], padding='VALID'),bias_weight) \n",
    "    \n",
    "    # RELU for Activation.\n",
    "    if activation == 1 :\n",
    "        cnl = tf.nn.relu(cnl)\n",
    "\n",
    "    return cnl\n",
    "def pooled_convlayer(x,conv_W,bias_weight):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    cnl    = tf.nn.bias_add(tf.nn.conv2d(x,conv_W, strides=[1, 1, 1, 1], padding='VALID'),bias_weight) \n",
    "    \n",
    "    # RELU for Activation.\n",
    "    cnl = tf.nn.relu(cnl)\n",
    "    \n",
    "    #Pooling\n",
    "    cnl = tf.nn.max_pool(cnl, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    return cnl\n",
    "\n",
    "def full_connection_layer(x,fully_W, bias_weight,activation = 1):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    fc   = tf.nn.bias_add(tf.matmul(x, fully_W) ,bias_weight)\n",
    "    \n",
    "    # RELU for Activation.\n",
    "    if activation == 1 :\n",
    "        fc = tf.nn.relu(fc)\n",
    "    \n",
    "    return fc\n",
    "def Lenet(x):\n",
    "    # Layer 1. Convolutional. Input = 32x32x1. Output = 14x14x6\n",
    "    cnv_layer_1 = pooled_convlayer(x,weights['wc1'],biases['bc1'])\n",
    "    # Layer 2 Convolutional. Input = 14x14x6. Output = 5x5x16\n",
    "    cnv_layer_2 = pooled_convlayer(cnv_layer_1, weights['wc2'],biases['bc2'])\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc_1 = tf.contrib.layers.flatten(cnv_layer_2)\n",
    "    fc_1 = full_connection_layer(fc_1,weights['wd1'],biases['bd1'])\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc_2 = full_connection_layer(fc_1,weights['wd2'],biases['bd2'])\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc_3 = full_connection_layer(fc_2,weights['wd3'],biases['bd3'],activation=0)\n",
    "    return fc_3\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "\n",
    "def Lenet3(x):\n",
    "    cnv_layer_1 = pooled_convlayer(x,weights['wc1'],biases['bc1'])\n",
    "    \n",
    "    cnv_layer_2 = pooled_convlayer(cnv_layer_1, weights['wc2'],biases['bc2'])\n",
    "\n",
    "    cnv_layer_1_beta = tf.nn.max_pool(cnv_layer_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    cnv_layer_1_gamma   = cnl_layer(cnv_layer_1_beta,weights['wc1_gamma'],biases['wc1_gamma'])\n",
    "    \n",
    "    cnv_layer_2_beta = cnl_layer(cnv_layer_2,weights['wc2_beta'],biases['wc2_beta'])\n",
    "    \n",
    "    new_conv = tf.concat([cnv_layer_1_gamma,cnv_layer_2_beta],1)\n",
    "    \n",
    "    fc_0 = tf.contrib.layers.flatten(new_conv)\n",
    "\n",
    "    fc_1 = full_connection_layer(fc_0,weights['bd_flat'],biases['bd1'])\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc_2 = full_connection_layer(fc_1,weights['wd2'],biases['bd2'])\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc_3 = full_connection_layer(fc_2,weights['wd3'],biases['bd3'],activation=0)\n",
    "    return fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = 0.0001\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1),name='x')\n",
    "y = tf.placeholder(tf.int32, (None),name = 'y')\n",
    "one_hot_y  = tf.one_hot(y,43)\n",
    "logits = Lenet3(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels = one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate = rate) \n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaled_img2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3387555acbe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_img2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_img2' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    #saver = tf.train.import_meta_graph('lenet.meta')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(10):\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = scaled_img2[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        training_accuracy = evaluate(scaled_img2,y_train)    \n",
    "        validation_accuracy = evaluate(X_valid_scaled2, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy),\n",
    "              \"Traing Accuracy  = {:.3f}\".format(training_accuracy))\n",
    "        print()\n",
    "\n",
    "    saver.save(sess, './test')\n",
    "    print(\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
